import feedparser
import re
import json
import requests
from datetime import datetime
from config import Config
# [NEW] Playwright ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ì„¤ì¹˜ í•„ìš”: pip install playwright && playwright install)
from playwright.sync_api import sync_playwright

class NewsAgent:
    def __init__(self):
        pass

    def clean_html(self, raw_html):
        """RSS Feedìš©: HTML íƒœê·¸ ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°"""
        # 1. ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ íƒœê·¸ ë‚´ìš© ì œê±°
        script_pattern = re.compile(r'<(script|style).*?>.*?</\1>', re.DOTALL)
        text = re.sub(script_pattern, ' ', raw_html)
        
        # 2. HTML íƒœê·¸ ì œê±°
        cleanr = re.compile('<.*?>')
        cleantext = re.sub(cleanr, ' ', text)
        
        # 3. ë‹¤ì¤‘ ê³µë°± ì œê±°
        return " ".join(cleantext.split())

    # =========================================================================
    # [Option 1] Google News RSS (ìœ ì§€)
    # =========================================================================
    def get_google_news_rss(self, category="world"):
        print(f"ğŸ“¡ [News] Attempting Primary Source: Google News RSS ({category.upper()})...")
        
        base_url = "https://news.google.com/rss"
        rss_urls = {
            "world": f"{base_url}/headlines/section/topic/WORLD?hl=en-US&gl=US&ceid=US:en",
            "tech": f"{base_url}/headlines/section/topic/TECHNOLOGY?hl=en-US&gl=US&ceid=US:en",
            "finance": f"{base_url}/headlines/section/topic/BUSINESS?hl=en-US&gl=US&ceid=US:en",
            "sports": f"{base_url}/headlines/section/topic/SPORTS?hl=en-US&gl=US&ceid=US:en",
            "ent": f"{base_url}/headlines/section/topic/ENTERTAINMENT?hl=en-US&gl=US&ceid=US:en",
            "art": f"{base_url}/search?q=Arts+Culture+Design&hl=en-US&gl=US&ceid=US:en"
        }

        try:
            target_url = rss_urls.get(category, rss_urls["world"])
            feed = feedparser.parse(target_url)
            
            if not feed.entries:
                print("   âš ï¸ RSS feed empty. Switching to backup...")
                return None

            top_entries = feed.entries[:8]
            news_context = f"Top {len(top_entries)} Headlines for {category.upper()} News ({datetime.now().strftime('%Y-%m-%d')}):\n\n"

            for i, entry in enumerate(top_entries):
                title = entry.title
                desc = self.clean_html(entry.description) if 'description' in entry else ""
                
                news_context += f"{i+1}. {title}\n"
                news_context += f"   - Snippet: {desc[:200]}...\n\n"
                print(f"   ğŸ“– [RSS] Item {i+1}: {title[:40]}...")

            return news_context

        except Exception as e:
            print(f"   âš ï¸ RSS Error: {e}")
            return None

    # =========================================================================
    # [Option 2] Serper Search & Snippet (ìœ ì§€)
    # =========================================================================
    def get_serper_backup(self, category="world"):
        print(f"ğŸ” [News] Attempting Secondary Source: Serper Search ({category.upper()})...")
        
        url = "https://google.serper.dev/search"
        query_map = {
            "world": "top world news today",
            "tech": "latest technology news today",
            "finance": "top finance business news today",
            "art": "latest arts and culture news today",
            "sports": "top sports news headlines today",
            "ent": "entertainment news headlines today"
        }
        
        query = query_map.get(category, "latest news today")
        payload = json.dumps({"q": query, "num": 10})
        headers = {'X-API-KEY': Config.SERPER_KEY, 'Content-Type': 'application/json'}

        try:
            response = requests.post(url, headers=headers, data=payload)
            results = response.json()
            
            if "organic" not in results:
                print("   âŒ Backup search failed.")
                return None
                
            items = results["organic"]
            news_context = f"[BACKUP SOURCE] Search Results for {category.upper()} News:\n\n"
            
            count = 0
            for item in items:
                if count >= 8: break
                title = item.get("title", "")
                snippet = item.get("snippet", "")
                link = item.get("link", "")
                
                if len(title) < 5: continue
                
                news_context += f"{count+1}. {title}\n"
                news_context += f"   - Snippet: {snippet}\n"
                news_context += f"   - Source: {link}\n\n"
                print(f"   ğŸ“– [Backup] Item {count+1}: {title[:40]}...")
                count += 1
                
            return news_context

        except Exception as e:
            print(f"   âŒ Backup Error: {e}")
            return None

    # =========================================================================
    # ë©”ì¸ í˜¸ì¶œ í•¨ìˆ˜ (Main Entry Point)
    # =========================================================================
    def get_daily_news(self, category="world"):
        # 1. RSS ì‹œë„
        context = self.get_google_news_rss(category)
        
        # 2. ì‹¤íŒ¨ì‹œ ë°±ì—… ì‹œë„
        if not context:
            print("âš ï¸ Primary (RSS) failed. Using Backup (Serper)...")
            context = self.get_serper_backup(category)
            
        if not context:
            print("âŒ All news sources failed.")
            return None
            
        return context

    def get_specific_news(self, url):
        """
        [UPGRADED] Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ì ‘ì† í›„ ë³¸ë¬¸ ì¶”ì¶œ
        """
        print(f"ğŸ”— [News] Deep Analyzing specific URL with Playwright: {url}")
        
        try:
            # Playwright ë¸Œë¼ìš°ì € ì‹¤í–‰
            with sync_playwright() as p:
                # headless=True: ë¸Œë¼ìš°ì € ì°½ì„ ë„ìš°ì§€ ì•Šê³  ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ (ë¹ ë¦„)
                # headless=False: ë¸Œë¼ìš°ì €ê°€ ëœ¨ëŠ” ê²ƒì„ ëˆˆìœ¼ë¡œ í™•ì¸ ê°€ëŠ¥ (ë””ë²„ê¹…ìš©)
                browser = p.chromium.launch(headless=True)
                
                # ëª¨ë°”ì¼ ë·°í¬íŠ¸ë‚˜ íŠ¹ì • User-Agentë¥¼ ì„¤ì •í•˜ì—¬ ë´‡ íƒì§€ íšŒí”¼ ê°€ëŠ¥ì„± ë†’ì„
                context = browser.new_context(
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    viewport={'width': 1920, 'height': 1080}
                )
                
                page = context.new_page()
                
                # í˜ì´ì§€ ì´ë™ (ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°)
                print("   â³ Loading page...")
                page.goto(url, timeout=30000, wait_until="domcontentloaded")
                
                # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (body íƒœê·¸ ë‚´ë¶€ì˜ ìˆœìˆ˜ í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜´)
                # inner_text()ëŠ” ìˆ¨ê²¨ì§„ ìš”ì†Œë‚˜ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì œì™¸í•˜ê³  ì‹¤ì œ ë³´ì´ëŠ” í…ìŠ¤íŠ¸ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
                content_text = page.locator("body").inner_text()
                
                browser.close()

                # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼
                if len(content_text) < 200:
                    raise Exception("Extracted content is too short (Block suspected).")

                # AI í† í° ì ˆì•½ì„ ìœ„í•´ 4000ì ì œí•œ
                final_text = content_text[:4000]
                # ë¶ˆí•„ìš”í•œ ì—°ì† ê³µë°± ì œê±°
                final_text = " ".join(final_text.split())

                print(f"   âœ… Content fetched successfully ({len(final_text)} chars)")
                return f"Source Article Content from {url}:\n\n{final_text}..."

        except Exception as e:
            print(f"   âŒ Browser Crawling Error: {e}")
            return f"User provided specific URL: {url}. (Crawling failed due to {e}, please generate script based on this link's context)."