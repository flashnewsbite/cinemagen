import feedparser
import re
import json
import requests
from datetime import datetime
from config import Config

class NewsAgent:
    def __init__(self):
        pass

    def clean_html(self, raw_html):
        """HTML íƒœê·¸ ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°"""
        # 1. ìŠ¤í¬ë¦½íŠ¸/ìŠ¤íƒ€ì¼ íƒœê·¸ ë‚´ìš© ì œê±° (ë³¸ë¬¸ê³¼ ì„ì´ì§€ ì•Šê²Œ)
        script_pattern = re.compile(r'<(script|style).*?>.*?</\1>', re.DOTALL)
        text = re.sub(script_pattern, ' ', raw_html)
        
        # 2. HTML íƒœê·¸ ì œê±°
        cleanr = re.compile('<.*?>')
        cleantext = re.sub(cleanr, ' ', text)
        
        # 3. ë‹¤ì¤‘ ê³µë°± ì œê±°
        return " ".join(cleantext.split())

    # =========================================================================
    # [Option 1] Google News RSS (1ìˆœìœ„: Daily Newsìš©)
    # =========================================================================
    def get_google_news_rss(self, category="world"):
        print(f"ğŸ“¡ [News] Attempting Primary Source: Google News RSS ({category.upper()})...")
        
        base_url = "https://news.google.com/rss"
        rss_urls = {
            "world": f"{base_url}/headlines/section/topic/WORLD?hl=en-US&gl=US&ceid=US:en",
            "tech": f"{base_url}/headlines/section/topic/TECHNOLOGY?hl=en-US&gl=US&ceid=US:en",
            "finance": f"{base_url}/headlines/section/topic/BUSINESS?hl=en-US&gl=US&ceid=US:en",
            "sports": f"{base_url}/headlines/section/topic/SPORTS?hl=en-US&gl=US&ceid=US:en",
            "ent": f"{base_url}/headlines/section/topic/ENTERTAINMENT?hl=en-US&gl=US&ceid=US:en",
            "art": f"{base_url}/search?q=Arts+Culture+Design&hl=en-US&gl=US&ceid=US:en"
        }

        try:
            target_url = rss_urls.get(category, rss_urls["world"])
            feed = feedparser.parse(target_url)
            
            if not feed.entries:
                print("   âš ï¸ RSS feed empty. Switching to backup...")
                return None

            top_entries = feed.entries[:8]
            news_context = f"Top {len(top_entries)} Headlines for {category.upper()} News ({datetime.now().strftime('%Y-%m-%d')}):\n\n"

            for i, entry in enumerate(top_entries):
                title = entry.title
                desc = self.clean_html(entry.description) if 'description' in entry else ""
                
                news_context += f"{i+1}. {title}\n"
                news_context += f"   - Snippet: {desc[:200]}...\n\n"
                print(f"   ğŸ“– [RSS] Item {i+1}: {title[:40]}...")

            return news_context

        except Exception as e:
            print(f"   âš ï¸ RSS Error: {e}")
            return None

    # =========================================================================
    # [Option 2] Serper Search & Snippet (2ìˆœìœ„: Daily News ë°±ì—…ìš©)
    # =========================================================================
    def get_serper_backup(self, category="world"):
        print(f"ğŸ” [News] Attempting Secondary Source: Serper Search ({category.upper()})...")
        
        url = "https://google.serper.dev/search"
        query_map = {
            "world": "top world news today",
            "tech": "latest technology news today",
            "finance": "top finance business news today",
            "art": "latest arts and culture news today",
            "sports": "top sports news headlines today",
            "ent": "entertainment news headlines today"
        }
        
        query = query_map.get(category, "latest news today")
        payload = json.dumps({"q": query, "num": 10})
        headers = {'X-API-KEY': Config.SERPER_KEY, 'Content-Type': 'application/json'}

        try:
            response = requests.post(url, headers=headers, data=payload)
            results = response.json()
            
            if "organic" not in results:
                print("   âŒ Backup search failed.")
                return None
                
            items = results["organic"]
            news_context = f"[BACKUP SOURCE] Search Results for {category.upper()} News:\n\n"
            
            count = 0
            for item in items:
                if count >= 8: break
                title = item.get("title", "")
                snippet = item.get("snippet", "")
                link = item.get("link", "")
                
                if len(title) < 5: continue
                
                news_context += f"{count+1}. {title}\n"
                news_context += f"   - Snippet: {snippet}\n"
                news_context += f"   - Source: {link}\n\n"
                print(f"   ğŸ“– [Backup] Item {count+1}: {title[:40]}...")
                count += 1
                
            return news_context

        except Exception as e:
            print(f"   âŒ Backup Error: {e}")
            return None

    # =========================================================================
    # ë©”ì¸ í˜¸ì¶œ í•¨ìˆ˜ (Main Entry Point)
    # =========================================================================
    def get_daily_news(self, category="world"):
        # 1. RSS ì‹œë„
        context = self.get_google_news_rss(category)
        
        # 2. ì‹¤íŒ¨ì‹œ ë°±ì—… ì‹œë„
        if not context:
            print("âš ï¸ Primary (RSS) failed. Using Backup (Serper)...")
            context = self.get_serper_backup(category)
            
        if not context:
            print("âŒ All news sources failed.")
            return None
            
        return context

    def get_specific_news(self, url):
        """
        [ê¸°ëŠ¥ ì¶”ê°€] íŠ¹ì • URLì˜ ë³¸ë¬¸ì„ ì§ì ‘ ê¸ì–´ì˜¤ëŠ” Deep Crawling
        """
        print(f"ğŸ”— [News] Deep Analyzing specific URL: {url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

        try:
            # 5ì´ˆ íƒ€ì„ì•„ì›ƒì„ ë‘ê³  ì ‘ì† ì‹œë„
            response = requests.get(url, headers=headers, timeout=10)
            
            # 403/404 ë“± ì—ëŸ¬ ì²´í¬
            if response.status_code != 200:
                print(f"   âš ï¸ URL Access Failed (Status: {response.status_code})")
                return f"User provided specific URL: {url}. (Access denied, please generate generic script based on this topic)."

            # HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ì œ
            clean_text = self.clean_html(response.text)
            
            # ë³¸ë¬¸ì´ ë„ˆë¬´ ê¸¸ë©´ AI í† í° ì ˆì•½ì„ ìœ„í•´ ì•ë¶€ë¶„ 4000ìë§Œ ì‚¬ìš©
            final_text = clean_text[:4000]
            
            print(f"   âœ… Content fetched successfully ({len(final_text)} chars)")
            return f"Source Article Content from {url}:\n\n{final_text}..."

        except Exception as e:
            print(f"   âŒ URL Crawling Error: {e}")
            # í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œì—ë„ ì—ëŸ¬ë¡œ ì£½ì§€ ì•Šê³ , URL ì£¼ì œë¡œ ëŒ€ë³¸ì„ ì“°ë„ë¡ ìœ ë„
            return f"User provided specific URL: {url}. (Crawling failed due to {e}, please generate script based on this link's context)."