import requests
import json
import xml.etree.ElementTree as ET
from newspaper import Article, Config as NewsConfig
from config import Config
from datetime import date
from urllib.parse import urlparse
import random
import time

class NewsAgent:
    TRUSTED_DOMAINS = [
        "cnn.com", "foxnews.com", "reuters.com", "bbc.com", "bbc.co.uk",
        "cbsnews.com", "abcnews.go.com", "usatoday.com", "newsweek.com",
        "bloomberg.com", "nbcnews.com", "apnews.com", "nytimes.com", 
        "washingtonpost.com", "wsj.com"
    ]

    # [í•µì‹¬] ë´‡ ì°¨ë‹¨ì„ ëš«ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì‹ ë¶„ì¦(User-Agents) ë¦¬ìŠ¤íŠ¸
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/115.0',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
        'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1'
    ]

    def get_daily_news(self):
        today = date.today().strftime("%Y-%m-%d")
        print(f"ğŸ“° [News] ì£¼ìš” ì–¸ë¡ ì‚¬ Deep Search ì‹œì‘... ({today})")
        
        if Config.SERPER_KEY:
            try:
                query = f"Top breaking news headlines U.S. and World {today}"
                url = "https://google.serper.dev/news"
                payload = json.dumps({
                    "q": query, "gl": "us", "hl": "en", "num": 20, "tbs": "qdr:d"
                })
                headers = {'X-API-KEY': Config.SERPER_KEY, 'Content-Type': 'application/json'}
                response = requests.post(url, headers=headers, data=payload, timeout=10)
                
                if response.status_code == 200:
                    data = response.json()
                    full_reports = []
                    crawled_count = 0
                    
                    print("   ğŸ‘‰ ê²€ìƒ‰ ì™„ë£Œ. ë©”ì´ì € ì–¸ë¡ ì‚¬ ìœ„ì£¼ë¡œ ì ‘ì† ì‹œë„ (Anti-Bot ìš°íšŒ)...")

                    for item in data.get("news", []):
                        if crawled_count >= 4: break
                        
                        link = item.get("link")
                        source = item.get("source", "").lower()
                        domain = urlparse(link).netloc.lower()
                        
                        is_trusted = False
                        for trusted in self.TRUSTED_DOMAINS:
                            if trusted in domain or trusted in source:
                                is_trusted = True
                                break
                        
                        if is_trusted:
                            print(f"      ğŸ“– Reading: {item.get('title')}...")
                            # [ì¤‘ìš”] ë´‡ ì°¨ë‹¨ ë°©ì§€ ì ìš©ëœ í¬ë¡¤ë§ í•¨ìˆ˜ í˜¸ì¶œ
                            article_content = self.get_news_from_url(link)
                            
                            if article_content:
                                full_reports.append(f"--- ARTICLE {crawled_count+1} ({item.get('source')}) ---\n{article_content}\n")
                                crawled_count += 1
                                # [ì¤‘ìš”] ë„ˆë¬´ ë¹¨ë¦¬ ì ‘ì†í•˜ë©´ ì°¨ë‹¨ë˜ë¯€ë¡œ 1~2ì´ˆ ì‰¬ì—ˆë‹¤ê°€ ë‹¤ìŒ ê¸°ì‚¬ë¡œ ì´ë™
                                time.sleep(random.uniform(1.0, 2.0))
                    
                    if full_reports:
                        return "\n".join(full_reports)
                    else:
                        print("   âš ï¸ Deep Search ì‹¤íŒ¨. ì¼ë°˜ ìš”ì•½ìœ¼ë¡œ ëŒ€ì²´.")
                        return "\n".join([f"- {i['title']}: {i['snippet']}" for i in data.get("news", [])[:5]])

            except Exception as e:
                print(f"   âš ï¸ Serper ì‹¤íŒ¨ ({e}) -> RSS ë°±ì—… ì‹¤í–‰")

        return self.get_rss_news()

    def get_rss_news(self):
        # (ê¸°ì¡´ RSS ì½”ë“œì™€ ë™ì¼í•˜ì§€ë§Œ, requestsì—ë„ í—¤ë” ì¶”ê°€)
        print("   ğŸ‘‰ RSS Feed (CNN/BBC) ë°±ì—… ì‹¤í–‰...")
        rss_urls = ["http://rss.cnn.com/rss/edition.rss", "http://feeds.bbci.co.uk/news/world/rss.xml"]
        news_items = []
        
        # RSS ìš”ì²­ ë•Œë„ ë´‡ ì°¨ë‹¨ ë°©ì§€ í—¤ë” ì‚¬ìš©
        headers = {'User-Agent': random.choice(self.USER_AGENTS)}
        
        for url in rss_urls:
            try:
                resp = requests.get(url, headers=headers, timeout=5)
                root = ET.fromstring(resp.content)
                count = 0
                for item in root.findall('.//item'):
                    title = item.find('title').text
                    desc = item.find('description').text
                    if desc: desc = desc.split('<')[0]
                    news_items.append(f"- {title}: {desc}")
                    count += 1
                    if count >= 5: break
                if news_items: break
            except: continue
            
        if not news_items: return "No news data available."
        return "\n".join(news_items)

    def get_news_from_url(self, url):
        """ê°•ë ¥í•œ ìœ„ì¥ìˆ ì´ ì ìš©ëœ í¬ë¡¤ëŸ¬"""
        
        # [í•µì‹¬] ë§¤ë²ˆ ì ‘ì†í•  ë•Œë§ˆë‹¤ ì‹ ë¶„(ë¸Œë¼ìš°ì €)ì„ ëœë¤ìœ¼ë¡œ ë°”ê¿ˆ
        random_user_agent = random.choice(self.USER_AGENTS)
        
        config = NewsConfig()
        config.browser_user_agent = random_user_agent
        config.request_timeout = 10
        
        try:
            article = Article(url, config=config)
            article.download()
            article.parse()
            
            if len(article.text) < 200: return None
            return f"HEADLINE: {article.title}\nFULL TEXT: {article.text[:3000]}..."
        except Exception as e:
            # 403 Forbidden(ì°¨ë‹¨ë¨) ì—ëŸ¬ê°€ ë‚˜ë©´ ì¡°ìš©íˆ ë„˜ì–´ê°
            # print(f"      âŒ ì ‘ì† ì°¨ë‹¨ë¨ (Skipping): {e}") 
            return None